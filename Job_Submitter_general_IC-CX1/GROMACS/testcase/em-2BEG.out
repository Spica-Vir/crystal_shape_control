date:                Sat  2 Jul 19:53:02 BST 2022
hostname:            cx3-12-2.cx3.hpc.ic.ac.uk
system:              Linux cx3-12-2.cx3.hpc.ic.ac.uk 4.18.0-348.20.1.el8_5.x86_64 #1 SMP Tue Mar 8 12:56:54 EST 2022 x86_64 x86_64 x86_64 GNU/Linux
user:                hz1420
input:               /rds/general/user/hz1420/home/gromacs-case/em-2BEG.in
output:              /rds/general/user/hz1420/home/gromacs-case/em-2BEG.out
executable script:   /rds/general/user/hz1420/home/etc/runGROMACS/run_exec
executable dir:      module load gromacs/2021.3-mpi
executable:          gmx_mpi
<qstat -f 5789075.pbs>
Connection timed out
qstat: cannot connect to server pbs (errno=110)
</qstat -f 5789075.pbs>
Found input data: /rds/general/user/hz1420/home/gromacs-case/em-2BEG.tpr
All files are synchonised.

Start the job
Job name: em-2BEG ID: 5789075.pbs


# mpiexec: MPI Program startup


# mpiexec: Running in job 5789075.pbs at Sat  2 Jul 19:55:17 BST 2022
# mpiexec: Fabric configuration:
# mpiexec: node class CX3
# mpiexec: libfabric provider verbs
# mpiexec: libfabric inferface eth0
# mpiexec: MPI-IO configuration on / gpfs
# mpiexec: full path to program is /apps/gromacs/2021.3-mpi/bin/gmx_mpi
# mpiexec: program arguments are: mdrun -s em-2BEG.tpr
# mpiexec: 24 ranks allocated via PBS select
# mpiexec: 1 OpenMP threads / rank allocated by PBS select
# mpiexec: 24 ranks per node
# mpiexec: There are 256 cores/node. 24 will be used for this job
# mpiexec: Job has shared use of the allocated nodes. Disabling process-pinning
# mpiexec: Node is shared. Disabling process pinning
# mpiexec: machinefile configured as:
cx3-12-2.cx3.hpc.ic.ac.uk:24
#
# mpiexec: Checking all nodes are ONLINE using ping:
# mpiexec: All nodes appear ONLINE
# mpiexec: Checking all nodes are ONLINE using ssh:
# cx3-12-2.cx3.hpc.ic.ac.uk : # mpiexec: Dynamic linking for /apps/gromacs/2021.3-mpi/bin/gmx_mpi:
	linux-vdso.so.1 (0x00001509c94bd000)
	libgromacs_mpi.so.6 => /apps/gromacs/2021.3-mpi/lib64/libgromacs_mpi.so.6 (0x00001509c7cde000)
	libmpi.so.12 => /apps/mpi/intel/2019.6.166/lib/release/libmpi.so.12 (0x00001509c6c44000)
	librt.so.1 => /lib64/librt.so.1 (0x00001509c6a3c000)
	libpthread.so.0 => /lib64/libpthread.so.0 (0x00001509c681c000)
	libdl.so.2 => /lib64/libdl.so.2 (0x00001509c6618000)
	libgomp.so.1 => /apps/gcc/9.3.0/lib64/libgomp.so.1 (0x00001509c63e2000)
	libstdc++.so.6 => /apps/gcc/9.3.0/lib64/libstdc++.so.6 (0x00001509c6008000)
	libm.so.6 => /lib64/libm.so.6 (0x00001509c5c86000)
	libgcc_s.so.1 => /apps/gcc/9.3.0/lib64/libgcc_s.so.1 (0x00001509c5a6e000)
	libc.so.6 => /lib64/libc.so.6 (0x00001509c56a9000)
	libblas.so.3 => /lib64/libblas.so.3 (0x00001509c5455000)
	liblapack.so.3 => /lib64/liblapack.so.3 (0x00001509c4bb4000)
	libfabric.so.1 => /apps/mpi/intel/2019.6.166/libfabric/lib/libfabric.so.1 (0x00001509c496f000)
	/lib64/ld-linux-x86-64.so.2 (0x00001509c9293000)
	libgfortran.so.5 => /apps/gcc/9.3.0/lib64/libgfortran.so.5 (0x00001509c44e1000)
	libquadmath.so.0 => /apps/gcc/9.3.0/lib/../lib64/libquadmath.so.0 (0x00001509c429b000)
# mpiexec: launch started at Sat  2 Jul 19:55:17 BST 2022
# mpiexec: launching program...
# mpiexec: /apps/gromacs/2021.3-mpi/bin/gmx_mpi mdrun -s em-2BEG.tpr
# mpiexec: PROGRAM OUTPUT FOLLOWS
(cx3-12-2.cx3.hpc.ic.ac.uk:0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23)

                 :-) GROMACS - gmx mdrun, 2021.3-UNCHECKED (-:

                            GROMACS is written by:
     Andrey Alekseenko              Emile Apol              Rossen Apostolov     
         Paul Bauer           Herman J.C. Berendsen           Par Bjelkmar       
       Christian Blau           Viacheslav Bolnykh             Kevin Boyd        
     Aldert van Buuren           Rudi van Drunen             Anton Feenstra      
    Gilles Gouaillardet             Alan Gray               Gerrit Groenhof      
       Anca Hamuraru            Vincent Hindriksen          M. Eric Irrgang      
      Aleksei Iupinov           Christoph Junghans             Joe Jordan        
    Dimitrios Karkoulis            Peter Kasson                Jiri Kraus        
      Carsten Kutzner              Per Larsson              Justin A. Lemkul     
       Viveca Lindahl            Magnus Lundborg             Erik Marklund       
        Pascal Merz             Pieter Meulenhoff            Teemu Murtola       
        Szilard Pall               Sander Pronk              Roland Schulz       
       Michael Shirts            Alexey Shvetsov             Alfons Sijbers      
       Peter Tieleman              Jon Vincent              Teemu Virolainen     
     Christian Wennberg            Maarten Wolf              Artem Zhmurov       
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2019, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2021.3-UNCHECKED
Executable:   /apps/gromacs/2021.3-mpi/bin/gmx_mpi
Data prefix:  /apps/gromacs/2021.3-mpi
Working dir:  /rds/general/ephemeral/user/hz1420/ephemeral/em-2BEG_5789075
Command line:
  gmx_mpi mdrun -s em-2BEG.tpr

Compiled SIMD: AVX_256, but for this host/run AVX2_256 might be better (see
log).
Reading file em-2BEG.tpr, VERSION 2021.3-UNCHECKED (single precision)
Using 24 MPI processes
Using 1 OpenMP thread per MPI process


NOTE: The number of threads is not equal to the number of (logical) cores
      and the -pin option is set to auto: will not pin threads to cores.
      This can lead to significant performance degradation.
      Consider using -pin on (and -pinoffset in case you run multiple jobs).

Steepest Descents:
   Tolerance (Fmax)   =  1.00000e+03
   Number of steps    =        50000

writing lowest energy coordinates.

Steepest Descents converged to Fmax < 1000 in 327 steps
Potential Energy  = -5.2535081e+05
Maximum force     =  9.0128442e+02 on atom 790
Norm of force     =  4.0150538e+01

GROMACS reminds you: "If you want to save your child from polio, you can pray or you can inoculate... choose science." (Carl Sagan)

# mpiexec: finished at Sat  2 Jul 19:55:26 BST 2022

List of saved files
TEMPORARY          SAVED
md.log             em-2BEG.log       131866   Jul 2 19:55
traj.trr           em-2BEG.trr       398604   Jul 2 19:55
confout.gro        em-2BEG.gro       1494429  Jul 2 19:55
 
Disk usage:
<df -h .>
Filesystem      Size  Used Avail Use% Mounted on
rds              14P   11P  3.3P  77% /rds
</df -h .>
By folders:
<du -m .>
2	/rds/general/ephemeral/user/hz1420/ephemeral/em-2BEG_5789075
</du -m .>
