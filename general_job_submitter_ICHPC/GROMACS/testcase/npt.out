date:                Thu 30 Jun 22:58:18 BST 2022
hostname:            cx3-12-8.cx3.hpc.ic.ac.uk
system:              Linux cx3-12-8.cx3.hpc.ic.ac.uk 4.18.0-348.20.1.el8_5.x86_64 #1 SMP Tue Mar 8 12:56:54 EST 2022 x86_64 x86_64 x86_64 GNU/Linux
user:                hz1420
input:               /rds/general/user/hz1420/home/test/npt.in
output:              /rds/general/user/hz1420/home/test/npt.out
executable script:   /rds/general/user/hz1420/home/etc/runGROMACS/run_exec
executable dir:      module load gromacs/2021.3-mpi
executable:          gmx_mpi
<qstat -f 5784787.pbs>
Connection timed out
qstat: cannot connect to server pbs (errno=110)
</qstat -f 5784787.pbs>
Found input data: /rds/general/user/hz1420/home/test/npt.tpr
All files are synchonised.

Start the job
Job name: npt ID: 5784787.pbs


# mpiexec: MPI Program startup


# mpiexec: Running in job 5784787.pbs at Thu 30 Jun 23:00:40 BST 2022
# mpiexec: Fabric configuration:
# mpiexec: node class CX3
# mpiexec: libfabric provider verbs
# mpiexec: libfabric inferface eth0
# mpiexec: MPI-IO configuration on / gpfs
# mpiexec: full path to program is /apps/gromacs/2021.3-mpi/bin/gmx_mpi
# mpiexec: program arguments are: mdrun -s npt
# mpiexec: 24 ranks allocated via PBS select
# mpiexec: 1 OpenMP threads / rank allocated by PBS select
# mpiexec: 24 ranks per node
# mpiexec: There are 256 cores/node. 24 will be used for this job
# mpiexec: Job has shared use of the allocated nodes. Disabling process-pinning
# mpiexec: Node is shared. Disabling process pinning
# mpiexec: machinefile configured as:
cx3-12-8.cx3.hpc.ic.ac.uk:24
#
# mpiexec: Checking all nodes are ONLINE using ping:
# mpiexec: All nodes appear ONLINE
# mpiexec: Checking all nodes are ONLINE using ssh:
# cx3-12-8.cx3.hpc.ic.ac.uk : # mpiexec: Dynamic linking for /apps/gromacs/2021.3-mpi/bin/gmx_mpi:
	linux-vdso.so.1 (0x00007ffcc419d000)
	libgromacs_mpi.so.6 => /apps/gromacs/2021.3-mpi/lib64/libgromacs_mpi.so.6 (0x0000153f193f1000)
	libmpi.so.12 => /apps/mpi/intel/2019.6.166/lib/release/libmpi.so.12 (0x0000153f18357000)
	librt.so.1 => /lib64/librt.so.1 (0x0000153f1814f000)
	libpthread.so.0 => /lib64/libpthread.so.0 (0x0000153f17f2f000)
	libdl.so.2 => /lib64/libdl.so.2 (0x0000153f17d2b000)
	libgomp.so.1 => /apps/gcc/9.3.0/lib64/libgomp.so.1 (0x0000153f17af5000)
	libstdc++.so.6 => /apps/gcc/9.3.0/lib64/libstdc++.so.6 (0x0000153f1771b000)
	libm.so.6 => /lib64/libm.so.6 (0x0000153f17399000)
	libgcc_s.so.1 => /apps/gcc/9.3.0/lib64/libgcc_s.so.1 (0x0000153f17181000)
	libc.so.6 => /lib64/libc.so.6 (0x0000153f16dbc000)
	libblas.so.3 => /lib64/libblas.so.3 (0x0000153f16b68000)
	liblapack.so.3 => /lib64/liblapack.so.3 (0x0000153f162c7000)
	libfabric.so.1 => /apps/mpi/intel/2019.6.166/libfabric/lib/libfabric.so.1 (0x0000153f16082000)
	/lib64/ld-linux-x86-64.so.2 (0x0000153f1a9a6000)
	libgfortran.so.5 => /apps/gcc/9.3.0/lib64/libgfortran.so.5 (0x0000153f15bf4000)
	libquadmath.so.0 => /apps/gcc/9.3.0/lib/../lib64/libquadmath.so.0 (0x0000153f159ae000)
# mpiexec: launch started at Thu 30 Jun 23:00:42 BST 2022
# mpiexec: launching program...
# mpiexec: /apps/gromacs/2021.3-mpi/bin/gmx_mpi mdrun -s npt
# mpiexec: PROGRAM OUTPUT FOLLOWS
(cx3-12-8.cx3.hpc.ic.ac.uk:0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23)

                 :-) GROMACS - gmx mdrun, 2021.3-UNCHECKED (-:

                            GROMACS is written by:
     Andrey Alekseenko              Emile Apol              Rossen Apostolov     
         Paul Bauer           Herman J.C. Berendsen           Par Bjelkmar       
       Christian Blau           Viacheslav Bolnykh             Kevin Boyd        
     Aldert van Buuren           Rudi van Drunen             Anton Feenstra      
    Gilles Gouaillardet             Alan Gray               Gerrit Groenhof      
       Anca Hamuraru            Vincent Hindriksen          M. Eric Irrgang      
      Aleksei Iupinov           Christoph Junghans             Joe Jordan        
    Dimitrios Karkoulis            Peter Kasson                Jiri Kraus        
      Carsten Kutzner              Per Larsson              Justin A. Lemkul     
       Viveca Lindahl            Magnus Lundborg             Erik Marklund       
        Pascal Merz             Pieter Meulenhoff            Teemu Murtola       
        Szilard Pall               Sander Pronk              Roland Schulz       
       Michael Shirts            Alexey Shvetsov             Alfons Sijbers      
       Peter Tieleman              Jon Vincent              Teemu Virolainen     
     Christian Wennberg            Maarten Wolf              Artem Zhmurov       
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2019, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2021.3-UNCHECKED
Executable:   /apps/gromacs/2021.3-mpi/bin/gmx_mpi
Data prefix:  /apps/gromacs/2021.3-mpi
Working dir:  /rds/general/ephemeral/user/hz1420/ephemeral/npt_5784787
Command line:
  gmx_mpi mdrun -s npt

Compiled SIMD: AVX_256, but for this host/run AVX2_256 might be better (see
log).
Reading file npt.tpr, VERSION 2021.3-UNCHECKED (single precision)
Changing nstlist from 5 to 100, rlist from 1.4 to 1.537

Using 24 MPI processes
Using 1 OpenMP thread per MPI process


NOTE: The number of threads is not equal to the number of (logical) cores
      and the -pin option is set to auto: will not pin threads to cores.
      This can lead to significant performance degradation.
      Consider using -pin on (and -pinoffset in case you run multiple jobs).
starting mdrun 'Gallium Rubidium Oxygen Manganese Argon Carbon Silicon t=   0.00000 in water'
50000 steps,    100.0 ps.

Writing final coordinates.


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 10.6%.
 The balanceable part of the MD step is 77%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 8.2%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Z 0 %
 Average PME mesh/force load: 0.434
 Part of the total run time spent waiting due to PP/PME imbalance: 8.4 %

NOTE: 8.2 % of the available CPU time was lost due to load imbalance
      in the domain decomposition.
      You can consider manually changing the decomposition (option -dd);
      e.g. by using fewer domains along the box dimension in which there is
      considerable inhomogeneity in the simulated system.
NOTE: 8.4 % performance was lost because the PME ranks
      had less work to do than the PP ranks.
      You might want to decrease the number of PME ranks
      or decrease the cut-off and the grid spacing.


               Core t (s)   Wall t (s)        (%)
       Time:     5625.588      234.400     2400.0
                 (ns/day)    (hour/ns)
Performance:       36.861        0.651

GROMACS reminds you: "I have no responsibility to live up to what others expect of me. That's their mistake, not my failing." (Richard Feynman)

# mpiexec: finished at Thu 30 Jun 23:04:39 BST 2022

List of saved files
TEMPORARY          SAVED
md.log             npt.log           65262    Jun 30 23:04
traj.trr           npt.trr           40651488 Jun 30 23:04
confout.gro        npt.gro           2291397  Jun 30 23:04
state.cpt          npt.CheckPoint/   798860   Jun 30 23:04
 
Disk usage:
<df -h .>
Filesystem      Size  Used Avail Use% Mounted on
rds              14P   11P  3.1P  78% /rds
</df -h .>
By folders:
<du -m .>
41	/rds/general/ephemeral/user/hz1420/ephemeral/npt_5784787
</du -m .>
