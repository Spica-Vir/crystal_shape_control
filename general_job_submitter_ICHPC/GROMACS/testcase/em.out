date:                Thu 30 Jun 22:55:52 BST 2022
hostname:            cx3-12-8.cx3.hpc.ic.ac.uk
system:              Linux cx3-12-8.cx3.hpc.ic.ac.uk 4.18.0-348.20.1.el8_5.x86_64 #1 SMP Tue Mar 8 12:56:54 EST 2022 x86_64 x86_64 x86_64 GNU/Linux
user:                hz1420
input:               /rds/general/user/hz1420/home/test/em.in
output:              /rds/general/user/hz1420/home/test/em.out
executable script:   /rds/general/user/hz1420/home/etc/runGROMACS/run_exec
executable dir:      module load gromacs/2021.3-mpi
executable:          gmx_mpi
<qstat -f 5784787.pbs>
Connection timed out
qstat: cannot connect to server pbs (errno=110)
</qstat -f 5784787.pbs>
Found input data: /rds/general/user/hz1420/home/test/em.tpr
All files are synchonised.

Start the job
Job name: em ID: 5784787.pbs


# mpiexec: MPI Program startup


# mpiexec: Running in job 5784787.pbs at Thu 30 Jun 22:58:03 BST 2022
# mpiexec: Fabric configuration:
# mpiexec: node class CX3
# mpiexec: libfabric provider verbs
# mpiexec: libfabric inferface eth0
# mpiexec: MPI-IO configuration on / gpfs
# mpiexec: full path to program is /apps/gromacs/2021.3-mpi/bin/gmx_mpi
# mpiexec: program arguments are: mdrun -s em
# mpiexec: 24 ranks allocated via PBS select
# mpiexec: 1 OpenMP threads / rank allocated by PBS select
# mpiexec: 24 ranks per node
# mpiexec: There are 256 cores/node. 24 will be used for this job
# mpiexec: Job has shared use of the allocated nodes. Disabling process-pinning
# mpiexec: Node is shared. Disabling process pinning
# mpiexec: machinefile configured as:
cx3-12-8.cx3.hpc.ic.ac.uk:24
#
# mpiexec: Checking all nodes are ONLINE using ping:
# mpiexec: All nodes appear ONLINE
# mpiexec: Checking all nodes are ONLINE using ssh:
# cx3-12-8.cx3.hpc.ic.ac.uk : # mpiexec: Dynamic linking for /apps/gromacs/2021.3-mpi/bin/gmx_mpi:
	linux-vdso.so.1 (0x00007ffc9d4c5000)
	libgromacs_mpi.so.6 => /apps/gromacs/2021.3-mpi/lib64/libgromacs_mpi.so.6 (0x0000147262dba000)
	libmpi.so.12 => /apps/mpi/intel/2019.6.166/lib/release/libmpi.so.12 (0x0000147261d20000)
	librt.so.1 => /lib64/librt.so.1 (0x0000147261b18000)
	libpthread.so.0 => /lib64/libpthread.so.0 (0x00001472618f8000)
	libdl.so.2 => /lib64/libdl.so.2 (0x00001472616f4000)
	libgomp.so.1 => /apps/gcc/9.3.0/lib64/libgomp.so.1 (0x00001472614be000)
	libstdc++.so.6 => /apps/gcc/9.3.0/lib64/libstdc++.so.6 (0x00001472610e4000)
	libm.so.6 => /lib64/libm.so.6 (0x0000147260d62000)
	libgcc_s.so.1 => /apps/gcc/9.3.0/lib64/libgcc_s.so.1 (0x0000147260b4a000)
	libc.so.6 => /lib64/libc.so.6 (0x0000147260785000)
	libblas.so.3 => /lib64/libblas.so.3 (0x0000147260531000)
	liblapack.so.3 => /lib64/liblapack.so.3 (0x000014725fc90000)
	libfabric.so.1 => /apps/mpi/intel/2019.6.166/libfabric/lib/libfabric.so.1 (0x000014725fa4b000)
	/lib64/ld-linux-x86-64.so.2 (0x000014726436f000)
	libgfortran.so.5 => /apps/gcc/9.3.0/lib64/libgfortran.so.5 (0x000014725f5bd000)
	libquadmath.so.0 => /apps/gcc/9.3.0/lib/../lib64/libquadmath.so.0 (0x000014725f377000)
# mpiexec: launch started at Thu 30 Jun 22:58:04 BST 2022
# mpiexec: launching program...
# mpiexec: /apps/gromacs/2021.3-mpi/bin/gmx_mpi mdrun -s em
# mpiexec: PROGRAM OUTPUT FOLLOWS
(cx3-12-8.cx3.hpc.ic.ac.uk:0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23)

                 :-) GROMACS - gmx mdrun, 2021.3-UNCHECKED (-:

                            GROMACS is written by:
     Andrey Alekseenko              Emile Apol              Rossen Apostolov     
         Paul Bauer           Herman J.C. Berendsen           Par Bjelkmar       
       Christian Blau           Viacheslav Bolnykh             Kevin Boyd        
     Aldert van Buuren           Rudi van Drunen             Anton Feenstra      
    Gilles Gouaillardet             Alan Gray               Gerrit Groenhof      
       Anca Hamuraru            Vincent Hindriksen          M. Eric Irrgang      
      Aleksei Iupinov           Christoph Junghans             Joe Jordan        
    Dimitrios Karkoulis            Peter Kasson                Jiri Kraus        
      Carsten Kutzner              Per Larsson              Justin A. Lemkul     
       Viveca Lindahl            Magnus Lundborg             Erik Marklund       
        Pascal Merz             Pieter Meulenhoff            Teemu Murtola       
        Szilard Pall               Sander Pronk              Roland Schulz       
       Michael Shirts            Alexey Shvetsov             Alfons Sijbers      
       Peter Tieleman              Jon Vincent              Teemu Virolainen     
     Christian Wennberg            Maarten Wolf              Artem Zhmurov       
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2019, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2021.3-UNCHECKED
Executable:   /apps/gromacs/2021.3-mpi/bin/gmx_mpi
Data prefix:  /apps/gromacs/2021.3-mpi
Working dir:  /rds/general/ephemeral/user/hz1420/ephemeral/em_5784787
Command line:
  gmx_mpi mdrun -s em

Compiled SIMD: AVX_256, but for this host/run AVX2_256 might be better (see
log).
Reading file em.tpr, VERSION 2021.3-UNCHECKED (single precision)
Using 24 MPI processes
Using 1 OpenMP thread per MPI process


NOTE: The number of threads is not equal to the number of (logical) cores
      and the -pin option is set to auto: will not pin threads to cores.
      This can lead to significant performance degradation.
      Consider using -pin on (and -pinoffset in case you run multiple jobs).

Steepest Descents:
   Tolerance (Fmax)   =  1.00000e+03
   Number of steps    =        50000

writing lowest energy coordinates.

Steepest Descents converged to Fmax < 1000 in 231 steps
Potential Energy  = -5.1705747e+05
Maximum force     =  8.5046045e+02 on atom 790
Norm of force     =  4.9598763e+01

GROMACS reminds you: "It Wouldn't Hurt to Wipe Once In a While" (Beavis and Butthead)

# mpiexec: finished at Thu 30 Jun 22:58:10 BST 2022

List of saved files
TEMPORARY          SAVED
md.log             em.log            98596    Jun 30 22:58
traj.trr           em.trr            398604   Jun 30 22:58
confout.gro        em.gro            1494429  Jun 30 22:58
 
Disk usage:
<df -h .>
Filesystem      Size  Used Avail Use% Mounted on
rds              14P   11P  3.1P  78% /rds
</df -h .>
By folders:
<du -m .>
2	/rds/general/ephemeral/user/hz1420/ephemeral/em_5784787
</du -m .>
